{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MODEL` is the huggingface repo that will be downloaded from.\n",
    "`CUDA` defines whether CUDA will be used for `input_ids` (significant speedup on NVIDIA GPUs).\n",
    "`USER` defines the user tag (usage defined in `ROLE`).\n",
    "`ASSISTANT` defines the assistant tag (usage defined in `ROLE`).\n",
    "`START_HEADER` defines the start of the header tag (usage defined in `ROLE`).\n",
    "`END_HEADER` defines the end of the header tag (usage defined in `ROLE`).\n",
    "`ROLE` is a function for concatenating the header tags.\n",
    "`IMAGE` defines the image tag, leave blank for non-vision models.\n",
    "`prompt` defines the beginning of the prompt, leave blank for no start tag.\n",
    "`image` should be untouched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL=\"unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit\"\n",
    "CUDA=True\n",
    "USER=\"user\"\n",
    "ASSISTANT=\"assistant\"\n",
    "START_HEADER=\"<|eot_id|><|start_header_id|>\"\n",
    "END_HEADER=\"<|end_header_id|>\\n\"\n",
    "ROLE = lambda str : START_HEADER + str + END_HEADER\n",
    "IMAGE=\"<|image|>\"\n",
    "prompt = \"<|begin_of_text|>\"\n",
    "image=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "from PIL import Image\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(MODEL)\n",
    "model = AutoModelForImageTextToText.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(msg, *args, **kwargs):\n",
    "    global prompt\n",
    "    global image\n",
    "\n",
    "    prompt += ROLE(USER)\n",
    "    for arg in args:\n",
    "        prompt += IMAGE\n",
    "        image.append(arg)\n",
    "    prompt += msg\n",
    "    prompt += ROLE(ASSISTANT)\n",
    "\n",
    "    inputs = None\n",
    "    if len(image) > 0:\n",
    "        inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "    else:\n",
    "        inputs = processor(text=prompt, return_tensors=\"pt\")\n",
    "\n",
    "    if CUDA:\n",
    "        inputs = inputs.to(\"cuda\")\n",
    "\n",
    "    generatedids = model.generate(inputs.input_ids, max_length=256, do_sample=True) # remove do_sample\n",
    "    output = processor.batch_decode(generatedids)[0]\n",
    "\n",
    "    print(output)\n",
    "    prompt += output[-1]\n",
    "\n",
    "    return output[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGEXPR = r\"[ABCDEFGH]:[\\\\/].+?\\..+? \"\n",
    "\n",
    "while True:\n",
    "    msg = input()\n",
    "\n",
    "    images = re.findall(REGEXPR, msg)\n",
    "    msg = re.sub(REGEXPR, \"\", msg)\n",
    "\n",
    "    output = generate(msg)\n",
    "\n",
    "    print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
